<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta name="description" content="Bayesian Macroeconometrics in R (BMR)">
    <meta name="author" content="Keith O'Hara">

    <meta name="keywords" content="Bayesian, Macroeconometrics, BMR, Economics, PhD student, NYU, New York University, Econometrics, R, C++, Cpp, Research" />

    <link rel="shortcut icon" type="image/x-icon" href="siteicon.ico">

    <title>BMR: bvartvp</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/modern-business.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- Additional Settings -->
    <link href="css/kthohr_settings.css" rel="stylesheet">

    <!-- Syntax Highlighter -->
    <script type="text/javascript" src="js/syntaxhighlighter.js"></script>
    <link type="text/css" rel="stylesheet" href="css/swift_theme.css">

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    
    ga('create', 'UA-93902857-1', 'auto');
    ga('send', 'pageview');

    </script>

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>

    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <script src="js/jquery.js"></script>
    <script>
        $(function(){
            $("#mynavbar").load("navbar.html")
            $("#bmrhead").load("bmr_header.html")
            $("#myfooter").load("footer.html")
        });
    </script>

</head>

<style>
pre {
    display: inline-block;
}
</style>

<body>

    <!-- Navigation -->
    <div id="mynavbar"></div>

    <!-- Page Content -->
    <div class="container">

        <!-- Page Heading/Breadcrumbs -->
        <div id="bmrhead"></div>

<br>

<div class="row">
    <div class="col-md-2"></div>
    <div class="col-md-5">
        <div class="panel panel-default">
            <div class="panel-heading">
                <a data-toggle="collapse" href="#collapse1"><h4><strong style="font-size: 120%;">BMR: BVAR with Time-Varying Parameters</strong></h4></a>
            </div>
            <div id="collapse1" class="panel-collapse collapse">
                <div class="panel-body">
                    <a href="#definition">Fields and Methods</a> <br>
                    <a href="#details">Details</a> <br>
                    <a href="#examples">Examples</a>
                </div>
            </div>
        </div>
    </div>
</div>

<!--  -->

<div class="row">
    <div class="col-md-2"></div>
    <div class="col-md-8">

<p>BVAR with time-varying coefficients.</p>

<hr style="height:2px;border-width:0;background-color:black">

<h3 style="text-align: left;" id="definition"><strong style="font-size: 100%;">Fields and Methods</strong></h3>

<br>

<p><strong>Instantiation:</strong></p>
<pre class="brush: R;">
# create a new object
bvar_obj <- new(bvartvp)
</pre>

<p><strong>Fields:</strong></p>
<ul>
    <li><code>cons_term</code> a logical value (TRUE/FALSE) indicating the presence of a constant term (intercept) in the model.</li>
    <li><code>p</code> lag order (integer).</li>
    <li><code>c_int</code> integer version of <code>cons_term</code>.</li>

    <br>

    <li><code>n</code> sample length.</li>
    <li><code>M</code> number of variables.</li>
    <!-- <li><code>n_ext_vars</code> number of exogenous variables.</li> -->
    <li><code>K</code> number of right-hand-side terms in each equation.</li>
    <li><code>tau</code> training sample length.</li>

    <!-- <br> -->

    <!-- <li><code>Y</code> a $(n-p) \times M$ data matrix.</li> -->
    <!-- <li><code>X</code> a $(n-p) \times K$ data matrix.</li> -->

    <br>

    <li><code>alpha_hat</code> OLS estimate of $\alpha = \text{vec}(\beta)$.</li>
    <li><code>Sigma_hat</code> OLS estimate of $\Sigma$.</li>

    <br>

    <li><code>alpha_pr_mean</code> prior mean of $\alpha = \text{vec}(\beta)$.</li>
    <li><code>alpha_pr_var</code> prior covariance matrix of $\alpha = \text{vec}(\beta)$.</li>

    <li><code>Sigma_pr_scale</code> prior scale matrix of $\Sigma$.</li>
    <li><code>Sigma_pr_dof</code> prior degrees of freedom of $\Sigma$.</li>

    <br>

    <li><code>alpha_pt_mean</code> posterior mean of $\alpha = \text{vec}(\beta)$.</li>
    <li><code>alpha_pt_var</code> posterior covariance matrix of $\alpha = \text{vec}(\beta)$.</li>

    <li><code>Q_pt_mean</code> posterior mean of $Q$.</li>
    <li><code>Q_pt_dof</code> posterior degrees of freedom of $Q$.</li>

    <li><code>Sigma_pt_mean</code> posterior mean of $\Sigma$.</li>
    <li><code>Sigma_pt_dof</code> posterior degrees of freedom of $\Sigma$.</li>

    <br>

    <li><code>alpha_draws</code> an array of posterior draws for $\alpha$.</li>
    <li><code>Q_draws</code> an array of posterior draws for $Q$.</li>
    <li><code>Sigma_draws</code> an array of posterior draws for $\Sigma$.</li>
</ul>

<hr>

<p><strong>Build:</strong> two methods,</p>
<pre class="brush: R;">
bvar_obj$build(data_endog,cons_term,p)
</pre>

<ul>
    <li><code>data_endog</code> a $n \times m$ matrix of endogenous varibles.</li>
    <!-- <li><code>data_exog</code> a $n \times q$ matrix of exogenous varibles.</li> -->
    <li><code>cons_term</code> a logical value (TRUE/FALSE) indicating the presence of a constant term (intercept) in the model.</li>
    <li><code>p</code> lag order (integer).</li>
</ul>

<p><strong>Prior:</strong></p>
<pre class="brush: R;">
bvar_obj$prior(tau,Xi_beta,Xi_Q,gamma_Q,Xi_Sigma,gamma_S)
</pre>

<ul>
    <li><code>tau</code> training sample length.</li>
    <li><code>Xi_beta</code> scaling value for the prior covariance matrix of $\beta$.</li>
    <li><code>Xi_Q</code> scaling value for the prior covariance matrix of $Q$.</li>
    <li><code>gamma_Q</code> prior degrees of freedom for $Q$.</li>
    <li><code>Xi_Sigma</code> scaling value for the prior covariance matrix of $\Sigma$.</li>
    <li><code>gamma_S</code> prior degrees of freedom for $\Sigma$.</li>
</ul>

<p><strong>Gibbs sampling:</strong></p>
<pre class="brush: R;">
bvar_obj$gibbs(n_draws,n_burnin)
</pre>

<ul>
    <li><code>n_draws</code> number of posterior draws.</li>
    <li><code>n_burnin</code> number of burnin draws.</li>
</ul>

<hr style="height:2px;border-width:0;background-color:black">

<h3 style="text-align: left;" id="details"><strong style="font-size: 100%;">Details</strong></h3>

<br>

The model is comprised of two series: the first describing the dynamics of our observable series in a familiar VAR-type fashion, and the second defining the evolution of the state (parameter) vector over time, which is assumed to be an unobserved random walk. 
We denote this by
\begin{align}
		Y_t &= X_t \beta_t + \varepsilon_{Y,t} \\
	\beta_t &= \beta_{t-1} + \varepsilon_{\beta,t}
\end{align}
where $\varepsilon_{Y,t} \sim \mathcal{N} (0,\Sigma)$ and $\varepsilon_{\beta ,t} \sim \mathcal{N} (0,Q)$. 

<br><br>

The joint posterior kernel for this model is
\begin{equation*}
		p(\beta_{1:T}, Q, \Sigma | Z, Y) \propto p(Y | Z, \beta_{1:T}, Q, \Sigma) p(\beta_{0}, Q, \Sigma)
\end{equation*}
where the prior densities for $Q$ and $\Sigma$ are standard, but note that we are placing a prior on the initial value of $\beta$. 

<br><br>

The prior densities are assumed to be independent, $p(\beta_{0}, Q, \Sigma) = p(\beta_0) p(\Sigma ) p(Q)$, and are given by
\begin{align}
		p(\beta_0 ) &= \mathcal{N} (\bar{\beta}, \Xi_{\beta}) \\
		p(\Sigma ) &= \mathcal{I} \mathcal{W} (\Xi_{\Sigma}, \gamma_\Sigma) \\
		p(Q ) &= \mathcal{I} \mathcal{W} (\Xi_{Q}, \gamma_Q)
\end{align}

For $p(\beta_0)$, we use a subset of $Y$, $Y_{1:\tau}$, as a training sample to initialise estimation of $\beta_{\tau+1:T}$. $\bar{\beta}$ becomes the OLS estimate of $\beta$,
\begin{equation*}
	\widehat{\beta} = \left( \sum_{t=1}^{\tau}  X_t^{\top} X_t \right)^{-1} \left(\sum_{t=1}^{\tau} X_t^{\top} Y_t \right),
\end{equation*}
$\Xi_\beta$ serves to scale the variance of the OLS estimate,
\begin{equation*}
		V \left(\widehat{\beta}\right) = \frac{1}{\tau - (1 + m \cdot p)} \left( \sum_{t=1}^{\tau} X_t^{\top} \left( \sum_{s=1}^\tau \varepsilon_{Y,s} \varepsilon_{Y,s}^{\top} \right)^{-1} X_t \right)^{-1},
\end{equation*}
and the location matrix of $Q$ becomes $\Xi_Q \cdot \tau$ times the variance of the OLS estimate. Thus, the three prior densities become:
\begin{align}
		p(\beta_0 ) &= \mathcal{N} \left( \widehat{\beta}, \Xi_{\beta} \cdot V(\widehat{\beta}) \right) \\
		p(\Sigma ) &= \mathcal{I} \mathcal{W} (\Xi_{\Sigma}, \gamma_\Sigma) \\
		p(Q ) &= \mathcal{I} \mathcal{W} \left( \Xi_{Q} \cdot \tau \cdot V(\widehat{\beta}), \gamma_Q \right)
\end{align}

The standard factorisation of the posterior density kernel applies, so we can implement a Gibbs-style sampler. The conditional posterior distributions of $\Sigma$ and $Q$ are
\begin{align}
		p(\Sigma | \beta_{1:T}, Q,Z,y) &= \mathcal{IW} \left( \Xi_{\Sigma} + \sum_{t=1}^T (y_t - X_t\beta_{t}) (y_t - X_t \beta_{t})^{\top} , T + \gamma_\Sigma \right) \\
		p(Q | \beta_{0:T}, \Sigma,Z,y) &= \mathcal{IW} \left( \Xi_{Q} + \sum_{t=1}^T (\Delta \beta_{t}) (\Delta \beta_{t})^{\top}, T + \gamma_Q \right)
\end{align}
where $\Delta$ is the first-difference operator. 

<br> <br>

The conditional posterior distribution of the coefficient matrix $\beta_{1:T}$, $p(\beta_{1:T} | \Sigma, Q, \mathcal{Y}^T)$, is a little more complicated, however. 

<br> <br>

Let $\mathcal{Y}^T = \{ \mathcal{Y}_t \}_{t=1}^T$ be the set of all observations, where a superscript denotes all information up to and including time $t$, whereas a subscript implies a specific observation at time $t$. We can factorise the joint conditional posterior density of $\beta_{1:T}$ to
\begin{equation}
		p(\beta_{1:T} | \Sigma, Q, \mathcal{Y}^T) = p(\beta_{T} | \Sigma, Q, \mathcal{Y}^T) \prod_{t=1}^{T-1} p(\beta_{t} | \beta_{t+1}, \Sigma, Q, \mathcal{Y}^t)
\end{equation}
where
\begin{align}
		p(\beta_{T} | \Sigma, Q, \mathcal{Y}^T) &= \mathcal{N} (\beta_{T}, P_{T}) \\
		p(\beta_{t} | \beta_{t+1}, \Sigma, Q, \mathcal{Y}^t) &= \mathcal{N} (\beta_{t | t+1}, P_{t | t+1})
\end{align}
are the conditional predictive densities, with
\begin{align*}
	\beta_{t | t-1} &= \mathbb{E} (\beta_t | \mathcal{Y}^{t-1} , \Sigma, Q) \\
		P_{t | t-1} &= V(\beta_t | \mathcal{Y}^{t-1} , \Sigma, Q)
\end{align*}
(See, for example, Carter and Kohn (1994) for details, and Cogley and Sargent (2005) and Primiceri (2005) for applications (and very detailed appendices).)

<hr>

For each sampling iteration, we evaluate $p(\beta_{1:T} | \Sigma, Q, \mathcal{Y}^T)$ using the Durbin and Koopman (2002) simulation smoother. The process is divided into four parts. 

<ol>
    <li> Using the form
        \begin{align}
		    Y_t &= X_t \beta_t + \varepsilon_{Y,t} \\
	        \beta_t &= \beta_{t-1} + \varepsilon_{\beta,t}
        \end{align}
        simulate a new series of $Y_t^{(+)}$ and $\beta^{(+)}$. Let $Y^{(*)} = Y - Y^{(+)}$.</li>
    <li>Use the Kalman filter to find $\widehat{\beta}_{1:T}$.
    <ul>
        <li> Initial values: $\beta_0$ and $P_0$.</li>
        <li> Find the residual, the residual covariance matrix, and the Kalman gain matrix:
            \begin{align*}
	            \epsilon_{t+1} &= Y_{t+1}^{(*)} - X_{t+1} \beta_{t | t} \\
	            F_{t+1} &= X_{t+1} P_{t | t} X_{t+1}^{\top} + \Sigma \\
		        K_{t+1} &= P_{t | t} X_{t+1}^{\top} F_{t+1}^{-1}
            \end{align*}
        respectively.</li>
        <li> Update the predicted $\beta_{t+1}$ and $P_{t+1}$ series with $t+1$ information,
            \begin{align*}
                \beta_{t+ 1 | t + 1} &= \beta_{t | t} + K_{t+1} \epsilon_{t+1} \\
		        P_{t +1 | t + 1} &= P_{t | t} (\mathbb{I} - K_{t+1} X_{t+1})^\top + Q
            \end{align*}</li>
    </ul>
    <li> Then, using the filtered series, calculate
        \begin{equation*}
		    r_{t-1} = X_t F_{t}^{-1} \epsilon_t + (\mathbb{I} - K_{t+1} X_{t+1})^\top r_t
        \end{equation*}
        such that $r_T = \mathbf{0}$.

        <br>

        A smoothed estimate of $\beta_{t+1}$ is then given by:
        \begin{equation*}
	        \widehat{\beta}_{t}^{(*)} = \widehat{\beta}_{t-1}^{(*)} + Q r_t
        \end{equation*}
    <li> The mean of the $h$-th draw is then
        \begin{equation*}
	        \beta^{(h)} = \widehat{\beta}^{(*)} + \beta^{(+)}
        \end{equation*}</li>
</li>

<br>

Proceeding to draw $Q^{(h)}$ and $\Sigma^{(h)}$ is standard.

<hr style="height:2px;border-width:0;background-color:black">

<h3 style="text-align: left;" id="examples"><strong style="font-size: 100%;">Example</strong></h3>

<br>

<pre class="brush: ruby;">
rm(list=ls())
library(BMR)

#

data(BMRVARData)
bvar_data <- data.matrix(USMacroData[,2:4])

#

tau <- 80

XiBeta <- 4
XiQ <- 0.005
gammaQ <- tau
XiSigma <- 1
gammaS = 4

which_irfs = c(17,53,89,125)

bvar_obj <- new(bvartvp)

#

bvar_obj$build(bvar_data,TRUE,1)
bvar_obj$prior(tau,XiBeta,XiQ,gammaQ,XiSigma,gammaS)
bvar_obj$gibbs(10000,5000)

IRF(bvar_obj,20,which_irfs,var_names=colnames(USMacroData),save=FALSE)
plot(bvar_obj,var_names=colnames(USMacroData),save=FALSE)
</pre>

<hr>

    </div>
    </div>
    </div>

    <div id="myfooter"></div>

    <!-- jQuery -->
    <!--<script src="js/jquery.js"></script>-->

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
